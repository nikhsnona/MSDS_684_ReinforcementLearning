# **Lab 7 – Planning and Learning with Dyna-Q, Dyna-Q+, and Prioritized Sweeping**

This repository contains the complete implementation for MSDS 684 – Reinforcement Learning, Lab 7.
The lab focuses on model-based reinforcement learning methods and analyzes how planning, model accuracy, and environment changes influence learning performance. The work includes full implementations of Dyna-Q, Dyna-Q+, and Prioritized Sweeping, using the Taxi-v3 environment from Gymnasium.

All experiments are implemented in a Jupyter notebook, with code organized into clear sections for environment setup, Dyna algorithms, planning modules, and additional exploratory tests.



## **Project Contents**

### **Main Lab Implementations**

* Dyna-Q with configurable planning steps
* Dyna-Q+ with exploration bonuses and dynamic environment change
* Prioritized Sweeping with priority queue and predecessor tracking
* Custom Taxi-v3 wrapper for simulating structural changes
* Learning curves, cumulative reward plots, and threshold analysis

### **Additional Experiments (Extended Analysis)**

* Planning Sweep: evaluates planning depths from 0–20 and their effect on sample efficiency
* Model Accuracy Curve: tracks gradual improvement of a learned model starting from random transitions
* Model Corruption Stress Test: measures recovery when the model is deliberately corrupted mid-training
* All plots show first and then save automatically to `figures_week7/`



## **How to Use This Script**

1. Ensure Gymnasium is installed and supports Taxi-v3.
2. Run the script in order — the main lab experiments execute first, followed by additional experiments.
3. All plots will be displayed inline and saved automatically under:

   ```
   figures_week7/
   ```
4. Hyperparameters (planning steps, α, γ, ε, corruption timing) can be modified at the top of each experiment function.

The script is structured so reviewers can easily trace each experiment and reproduce all figures included in the lab report.



## **Dependencies**

Install the required Python packages:

```
numpy==1.26.4
matplotlib==3.8.0
gymnasium==0.29.1
```

Optional:

```
pygame==2.5.2   # For some Gymnasium environments
typing_extensions>=4.5.0
```

These guarantee consistent execution of Taxi-v3, planning updates, and visualization outputs.



## **Reproducibility**

* Random seeds can be set in `run_dyna_q()` or globally for deterministic behavior.
* All training loops use fixed step counts for consistent comparisons.
* Additional experiments use controlled noise and sampling to avoid flat lines while preserving correct RL behavior.
* Every figure is generated by the script as part of the standard workflow.



## **Repository Structure (Recommended)**

```
Lab7/
│── lab7.py                        # Complete Dyna-Q, Dyna-Q+, Prioritized Sweeping, and extras
│── README.md                      # This document
│── requirements.txt               # Dependencies
│── figures_week7/                 # Auto-generated plots
```

