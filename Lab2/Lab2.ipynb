{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \"Use pytorch for the neural net and svm for the model comparison\"-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSDS 684 - Lab 2: Dynamic Programming on GridWorld and FrozenLake-v1\n",
    "\n",
    "- Custom GridWorldEnv (Gymnasium API)\n",
    "- Policy Evaluation (synchronous + in-place)\n",
    "- Policy Iteration\n",
    "- Value Iteration (synchronous + in-place)\n",
    "- Visualizations:\n",
    "  * Value function heatmaps\n",
    "  * Policy quiver plots (arrows)\n",
    "  * Convergence curves (delta vs iteration and wall-clock time)\n",
    "- Tests:\n",
    "  * Deterministic GridWorld\n",
    "  * Stochastic GridWorld\n",
    "  * FrozenLake-v1 using env.unwrapped.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GridWorld Experiments ===\n",
      "\n",
      "--- Config: deterministic ---\n",
      "Running Policy Iteration (in-place eval)...\n",
      "Running Policy Iteration (synchronous eval)...\n",
      "Running Value Iteration (in-place)...\n",
      "Running Value Iteration (synchronous)...\n",
      "PI in-place:  final |ΔV|=0.00e+00, sweeps=298\n",
      "PI sync:      final |ΔV|=0.00e+00, sweeps=393\n",
      "VI in-place:  final |ΔV|=0.00e+00, sweeps=7\n",
      "VI sync:      final |ΔV|=0.00e+00, sweeps=7\n",
      "\n",
      "--- Config: stochastic ---\n",
      "Running Policy Iteration (in-place eval)...\n",
      "Running Policy Iteration (synchronous eval)...\n",
      "Running Value Iteration (in-place)...\n",
      "Running Value Iteration (synchronous)...\n",
      "PI in-place:  final |ΔV|=5.94e-07, sweeps=324\n",
      "PI sync:      final |ΔV|=7.51e-07, sweeps=433\n",
      "VI in-place:  final |ΔV|=5.90e-07, sweeps=20\n",
      "VI sync:      final |ΔV|=7.51e-07, sweeps=27\n",
      "\n",
      "=== FrozenLake-v1 Experiments ===\n",
      "Running Value Iteration (in-place) on FrozenLake...\n",
      "Running Policy Iteration (in-place eval) on FrozenLake...\n",
      "FrozenLake VI: final |ΔV|=9.72e-07, sweeps=228\n",
      "FrozenLake PI: final |ΔV|=9.89e-07, sweeps=461\n",
      "\n",
      "All experiments completed. Check the 'outputs_dp_lab2' folder for plots.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "\n",
    "\n",
    "# Utility\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def ensure_dir(path: str):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# Custom GridWorld Environment (Gymnasium API)\n",
    "class GridWorldEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Simple GridWorld with configurable:\n",
    "    - size (rows, cols)\n",
    "    - terminal states and rewards\n",
    "    - obstacles\n",
    "    - step cost\n",
    "    - stochastic transitions (slip to perpendicular actions)\n",
    "    Gymnasium-style Discrete state/action spaces with tabular P(s,a,s',r,done).\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\"render_modes\": []}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        shape: Tuple[int, int] = (4, 4),\n",
    "        start_state: int = 0,\n",
    "        terminal_states: Dict[int, float] = None,\n",
    "        obstacle_states: List[int] = None,\n",
    "        step_cost: float = -0.04,\n",
    "        stochastic: bool = False,\n",
    "        intended_prob: float = 0.8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.rows, self.cols = shape\n",
    "        self.nS = self.rows * self.cols\n",
    "        self.nA = 4  # 0: up, 1: right, 2: down, 3: left\n",
    "\n",
    "        self.action_space = spaces.Discrete(self.nA)\n",
    "        self.observation_space = spaces.Discrete(self.nS)\n",
    "\n",
    "        self.start_state = start_state\n",
    "        self.current_state = start_state\n",
    "\n",
    "        if terminal_states is None:\n",
    "            # default: bottom-right is +1\n",
    "            terminal_states = {self.nS - 1: 1.0}\n",
    "        self.terminal_states = terminal_states\n",
    "\n",
    "        if obstacle_states is None:\n",
    "            obstacle_states = []\n",
    "        self.obstacle_states = set(obstacle_states)\n",
    "\n",
    "        self.step_cost = step_cost\n",
    "        self.stochastic = stochastic\n",
    "        self.intended_prob = intended_prob\n",
    "\n",
    "        # Build transition model P[s][a] = list of (prob, next_state, reward, done)\n",
    "        self.P: Dict[int, Dict[int, List[Tuple[float, int, float, bool]]]] = {}\n",
    "        self._build_transition_model()\n",
    "\n",
    "\n",
    "    # Helpers\n",
    "    def to_pos(self, s: int) -> Tuple[int, int]:\n",
    "        return s // self.cols, s % self.cols\n",
    "\n",
    "    def to_state(self, r: int, c: int) -> int:\n",
    "        return r * self.cols + c\n",
    "\n",
    "    def in_bounds(self, r: int, c: int) -> bool:\n",
    "        return 0 <= r < self.rows and 0 <= c < self.cols\n",
    "\n",
    "    def is_terminal(self, s: int) -> bool:\n",
    "        return s in self.terminal_states\n",
    "\n",
    "    def is_obstacle(self, s: int) -> bool:\n",
    "        return s in self.obstacle_states\n",
    "\n",
    "    def _move(self, s: int, a: int) -> int:\n",
    "        \"\"\"\n",
    "        Deterministic move from state s with action a.\n",
    "        If off-grid or into obstacle -> stay in same state.\n",
    "        \"\"\"\n",
    "        if self.is_terminal(s):\n",
    "            return s\n",
    "\n",
    "        r, c = self.to_pos(s)\n",
    "        if a == 0:  # up\n",
    "            nr, nc = r - 1, c\n",
    "        elif a == 1:  # right\n",
    "            nr, nc = r, c + 1\n",
    "        elif a == 2:  # down\n",
    "            nr, nc = r + 1, c\n",
    "        else:  # left\n",
    "            nr, nc = r, c - 1\n",
    "\n",
    "        if not self.in_bounds(nr, nc):\n",
    "            return s\n",
    "\n",
    "        ns = self.to_state(nr, nc)\n",
    "        if self.is_obstacle(ns):\n",
    "            return s\n",
    "        return ns\n",
    "\n",
    "    def _build_transition_model(self):\n",
    "        self.P = {s: {a: [] for a in range(self.nA)} for s in range(self.nS)}\n",
    "\n",
    "        for s in range(self.nS):\n",
    "            if self.is_obstacle(s):\n",
    "                # Agent should never be in obstacle, but make it absorbing just in case.\n",
    "                for a in range(self.nA):\n",
    "                    self.P[s][a] = [(1.0, s, 0.0, False)]\n",
    "                continue\n",
    "\n",
    "            if self.is_terminal(s):\n",
    "                reward = self.terminal_states[s]\n",
    "                for a in range(self.nA):\n",
    "                    # Terminal absorbing state\n",
    "                    self.P[s][a] = [(1.0, s, reward, True)]\n",
    "                continue\n",
    "\n",
    "            for a in range(self.nA):\n",
    "                transitions = []\n",
    "\n",
    "                if not self.stochastic:\n",
    "                    ns = self._move(s, a)\n",
    "                    reward = self.step_cost\n",
    "                    if self.is_terminal(ns):\n",
    "                        reward += self.terminal_states[ns]\n",
    "                    done = self.is_terminal(ns)\n",
    "                    transitions.append((1.0, ns, reward, done))\n",
    "                else:\n",
    "                    # Stochastic: intended direction with p, perpendicular with remaining prob\n",
    "                    intended = a\n",
    "                    if a == 0 or a == 2:  # up/down -> slip left/right\n",
    "                        perpendiculars = [3, 1]  # left, right\n",
    "                    else:  # left/right -> slip up/down\n",
    "                        perpendiculars = [0, 2]  # up, down\n",
    "\n",
    "                    probs = [self.intended_prob,\n",
    "                             (1 - self.intended_prob) / 2,\n",
    "                             (1 - self.intended_prob) / 2]\n",
    "                    actions = [intended] + perpendiculars\n",
    "\n",
    "                    for pa, aa in zip(probs, actions):\n",
    "                        ns = self._move(s, aa)\n",
    "                        reward = self.step_cost\n",
    "                        if self.is_terminal(ns):\n",
    "                            reward += self.terminal_states[ns]\n",
    "                        done = self.is_terminal(ns)\n",
    "                        transitions.append((pa, ns, reward, done))\n",
    "\n",
    "                self.P[s][a] = transitions\n",
    "\n",
    "\n",
    "    # Gymnasium API\n",
    "    def reset(self, seed: int = None, options: dict = None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_state = self.start_state\n",
    "        return self.current_state, {}\n",
    "\n",
    "    def step(self, action: int):\n",
    "        transitions = self.P[self.current_state][action]\n",
    "        probs = [t[0] for t in transitions]\n",
    "        idx = np.random.choice(len(transitions), p=probs)\n",
    "        prob, next_state, reward, done = transitions[idx]\n",
    "        self.current_state = next_state\n",
    "        return next_state, reward, done, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        # Simple text render if you ever want it\n",
    "        grid = np.full((self.rows, self.cols), \" . \")\n",
    "        for s, r in self.terminal_states.items():\n",
    "            r_pos, c_pos = self.to_pos(s)\n",
    "            grid[r_pos, c_pos] = \"T+ \" if r > 0 else \"T- \"\n",
    "        for s in self.obstacle_states:\n",
    "            r_pos, c_pos = self.to_pos(s)\n",
    "            grid[r_pos, c_pos] = \"###\"\n",
    "        cr, cc = self.to_pos(self.current_state)\n",
    "        grid[cr, cc] = \" A \"\n",
    "        print(\"\\n\".join(\"\".join(row) for row in grid))\n",
    "\n",
    "\n",
    "\n",
    "# DP Helpers\n",
    "\n",
    "def policy_evaluation(\n",
    "    P: Dict[int, Dict[int, List[Tuple[float, int, float, bool]]]],\n",
    "    nS: int,\n",
    "    nA: int,\n",
    "    policy: np.ndarray,\n",
    "    gamma: float = 0.99,\n",
    "    theta: float = 1e-6,\n",
    "    max_iterations: int = 1000,\n",
    "    in_place: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Policy Evaluation for a given policy π(s,a).\n",
    "    policy: shape (nS, nA), row-stochastic.\n",
    "\n",
    "    Returns:\n",
    "        V: value function\n",
    "        V_history: list of V over iterations\n",
    "        deltas: max change per iteration\n",
    "        times: cumulative wall-clock times\n",
    "    \"\"\"\n",
    "    V = np.zeros(nS)\n",
    "    V_history = []\n",
    "    deltas = []\n",
    "    times = []\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    for it in range(max_iterations):\n",
    "        delta = 0.0\n",
    "\n",
    "        if in_place:\n",
    "            # Update V in-place\n",
    "            for s in range(nS):\n",
    "                v_old = V[s]\n",
    "                v_new = 0.0\n",
    "                for a in range(nA):\n",
    "                    action_prob = policy[s, a]\n",
    "                    if action_prob == 0.0:\n",
    "                        continue\n",
    "                    for prob, ns, reward, done in P[s][a]:\n",
    "                        v_new += action_prob * prob * (\n",
    "                            reward + gamma * (0.0 if done else V[ns])\n",
    "                        )\n",
    "                V[s] = v_new\n",
    "                delta = max(delta, abs(v_old - v_new))\n",
    "        else:\n",
    "            # Synchronous: use copy of previous V\n",
    "            V_new = np.zeros_like(V)\n",
    "            for s in range(nS):\n",
    "                v_new = 0.0\n",
    "                for a in range(nA):\n",
    "                    action_prob = policy[s, a]\n",
    "                    if action_prob == 0.0:\n",
    "                        continue\n",
    "                    for prob, ns, reward, done in P[s][a]:\n",
    "                        v_new += action_prob * prob * (\n",
    "                            reward + gamma * (0.0 if done else V[ns])\n",
    "                        )\n",
    "                V_new[s] = v_new\n",
    "                delta = max(delta, abs(V[s] - v_new))\n",
    "            V = V_new\n",
    "\n",
    "        V_history.append(V.copy())\n",
    "        deltas.append(delta)\n",
    "        times.append(time.time() - t0)\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return V, V_history, np.array(deltas), np.array(times)\n",
    "\n",
    "\n",
    "def greedy_policy_from_V(\n",
    "    P: Dict[int, Dict[int, List[Tuple[float, int, float, bool]]]],\n",
    "    nS: int,\n",
    "    nA: int,\n",
    "    V: np.ndarray,\n",
    "    gamma: float = 0.99,\n",
    "):\n",
    "    policy = np.zeros((nS, nA))\n",
    "    for s in range(nS):\n",
    "        q_values = np.zeros(nA)\n",
    "        for a in range(nA):\n",
    "            for prob, ns, reward, done in P[s][a]:\n",
    "                q_values[a] += prob * (reward + gamma * (0.0 if done else V[ns]))\n",
    "        best_actions = np.where(q_values == np.max(q_values))[0]\n",
    "        # Tie-break: pick first best action\n",
    "        best_a = best_actions[0]\n",
    "        policy[s, best_a] = 1.0\n",
    "    return policy\n",
    "\n",
    "\n",
    "def policy_improvement(\n",
    "    P: Dict[int, Dict[int, List[Tuple[float, int, float, bool]]]],\n",
    "    nS: int,\n",
    "    nA: int,\n",
    "    V: np.ndarray,\n",
    "    gamma: float,\n",
    "    policy: np.ndarray,\n",
    "):\n",
    "    new_policy = np.zeros_like(policy)\n",
    "    policy_stable = True\n",
    "\n",
    "    for s in range(nS):\n",
    "        old_action = np.argmax(policy[s])\n",
    "\n",
    "        q_values = np.zeros(nA)\n",
    "        for a in range(nA):\n",
    "            for prob, ns, reward, done in P[s][a]:\n",
    "                q_values[a] += prob * (reward + gamma * (0.0 if done else V[ns]))\n",
    "\n",
    "        best_actions = np.where(q_values == np.max(q_values))[0]\n",
    "        best_a = best_actions[0]\n",
    "\n",
    "        new_policy[s, best_a] = 1.0\n",
    "\n",
    "        if best_a != old_action:\n",
    "            policy_stable = False\n",
    "\n",
    "    return new_policy, policy_stable\n",
    "\n",
    "\n",
    "def policy_iteration(\n",
    "    P: Dict[int, Dict[int, List[Tuple[float, int, float, bool]]]],\n",
    "    nS: int,\n",
    "    nA: int,\n",
    "    gamma: float = 0.99,\n",
    "    theta: float = 1e-6,\n",
    "    max_policy_iterations: int = 100,\n",
    "    eval_in_place: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Classical Policy Iteration:\n",
    "    - Policy Evaluation (with either in-place or synchronous sweeps)\n",
    "    - Policy Improvement\n",
    "\n",
    "    Returns:\n",
    "        V\n",
    "        policy\n",
    "        V_history_per_pi: list of V (one per outer PI iteration)\n",
    "        policy_history: list of policies\n",
    "        deltas_eval: concatenated deltas from evaluation sweeps\n",
    "        times_eval: concatenated times from evaluation sweeps\n",
    "    \"\"\"\n",
    "    # Initialize random deterministic policy\n",
    "    policy = np.ones((nS, nA)) / nA\n",
    "\n",
    "    V_history_per_pi = []\n",
    "    policy_history = []\n",
    "    deltas_eval_all = []\n",
    "    times_eval_all = []\n",
    "\n",
    "    t0_global = time.time()\n",
    "\n",
    "    for it in range(max_policy_iterations):\n",
    "        # Policy Evaluation\n",
    "        V, V_hist_eval, deltas_eval, times_eval = policy_evaluation(\n",
    "            P=P,\n",
    "            nS=nS,\n",
    "            nA=nA,\n",
    "            policy=policy,\n",
    "            gamma=gamma,\n",
    "            theta=theta,\n",
    "            in_place=eval_in_place,\n",
    "        )\n",
    "\n",
    "        V_history_per_pi.append(V.copy())\n",
    "        policy_history.append(policy.copy())\n",
    "        deltas_eval_all.extend(deltas_eval.tolist())\n",
    "        # Shift times by global start so later we can compare wall-clock\n",
    "        times_eval_all.extend((times_eval + (time.time() - t0_global - times_eval[-1])).tolist())\n",
    "\n",
    "        # Policy Improvement\n",
    "        new_policy, policy_stable = policy_improvement(\n",
    "            P=P, nS=nS, nA=nA, V=V, gamma=gamma, policy=policy\n",
    "        )\n",
    "\n",
    "        policy = new_policy\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return (\n",
    "        V,\n",
    "        policy,\n",
    "        V_history_per_pi,\n",
    "        policy_history,\n",
    "        np.array(deltas_eval_all),\n",
    "        np.array(times_eval_all),\n",
    "    )\n",
    "\n",
    "\n",
    "def value_iteration(\n",
    "    P: Dict[int, Dict[int, List[Tuple[float, int, float, bool]]]],\n",
    "    nS: int,\n",
    "    nA: int,\n",
    "    gamma: float = 0.99,\n",
    "    theta: float = 1e-6,\n",
    "    max_iterations: int = 1000,\n",
    "    in_place: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Value Iteration:\n",
    "    - Either in-place (updating V(s) directly) or synchronous (using a copy).\n",
    "\n",
    "    Returns:\n",
    "        V\n",
    "        policy (greedy)\n",
    "        V_history: list of V\n",
    "        policy_history: list of greedy policies at each iteration\n",
    "        deltas: max change per iteration\n",
    "        times: cumulative wall-clock times\n",
    "    \"\"\"\n",
    "    V = np.zeros(nS)\n",
    "    V_history = []\n",
    "    policy_history = []\n",
    "    deltas = []\n",
    "    times = []\n",
    "    t0 = time.time()\n",
    "\n",
    "    for it in range(max_iterations):\n",
    "        delta = 0.0\n",
    "\n",
    "        if in_place:\n",
    "            for s in range(nS):\n",
    "                v_old = V[s]\n",
    "                q_values = np.zeros(nA)\n",
    "                for a in range(nA):\n",
    "                    for prob, ns, reward, done in P[s][a]:\n",
    "                        q_values[a] += prob * (reward + gamma * (0.0 if done else V[ns]))\n",
    "                V[s] = np.max(q_values)\n",
    "                delta = max(delta, abs(v_old - V[s]))\n",
    "        else:\n",
    "            V_new = np.zeros_like(V)\n",
    "            for s in range(nS):\n",
    "                q_values = np.zeros(nA)\n",
    "                for a in range(nA):\n",
    "                    for prob, ns, reward, done in P[s][a]:\n",
    "                        q_values[a] += prob * (reward + gamma * (0.0 if done else V[ns]))\n",
    "                V_new[s] = np.max(q_values)\n",
    "                delta = max(delta, abs(V[s] - V_new[s]))\n",
    "            V = V_new\n",
    "\n",
    "        V_history.append(V.copy())\n",
    "        deltas.append(delta)\n",
    "        times.append(time.time() - t0)\n",
    "\n",
    "        # Greedy policy for visualization at each iteration\n",
    "        policy = greedy_policy_from_V(P, nS, nA, V, gamma)\n",
    "        policy_history.append(policy.copy())\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    final_policy = greedy_policy_from_V(P, nS, nA, V, gamma)\n",
    "    return V, final_policy, V_history, policy_history, np.array(deltas), np.array(times)\n",
    "\n",
    "\n",
    "\n",
    "# Visualization Utilities\n",
    "ACTION_ARROWS = {\n",
    "    0: (0, -1),   # up: pointing upwards in row/col coordinate\n",
    "    1: (1, 0),    # right\n",
    "    2: (0, 1),    # down\n",
    "    3: (-1, 0),   # left\n",
    "}\n",
    "\n",
    "\n",
    "def plot_value_heatmap(\n",
    "    V: np.ndarray,\n",
    "    rows: int,\n",
    "    cols: int,\n",
    "    title: str,\n",
    "    out_path: str,\n",
    "):\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    grid = V.reshape(rows, cols)\n",
    "    plt.imshow(grid, cmap=\"viridis\", origin=\"upper\")\n",
    "    plt.colorbar(label=\"V(s)\")\n",
    "    plt.title(title)\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            plt.text(c, r, f\"{grid[r, c]:.2f}\", ha=\"center\", va=\"center\", fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_policy_arrows(\n",
    "    policy: np.ndarray,\n",
    "    rows: int,\n",
    "    cols: int,\n",
    "    title: str,\n",
    "    out_path: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    policy: shape (nS, nA), one-hot per state.\n",
    "    \"\"\"\n",
    "    nS, nA = policy.shape\n",
    "    assert nS == rows * cols\n",
    "\n",
    "    X, Y, U, V = [], [], [], []\n",
    "\n",
    "    for s in range(nS):\n",
    "        r, c = divmod(s, cols)\n",
    "        a = np.argmax(policy[s])\n",
    "        dx, dy = ACTION_ARROWS[a]\n",
    "        # In image coordinates, x=col, y=row\n",
    "        X.append(c)\n",
    "        Y.append(r)\n",
    "        U.append(dx)\n",
    "        V.append(dy)\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.quiver(X, Y, U, V, angles=\"xy\", scale_units=\"xy\", scale=1)\n",
    "    plt.xticks(range(cols))\n",
    "    plt.yticks(range(rows))\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_convergence_curves(\n",
    "    results: Dict[str, Tuple[np.ndarray, np.ndarray]],\n",
    "    title: str,\n",
    "    out_dir: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    results: dict name -> (deltas, times)\n",
    "    Saves two plots:\n",
    "        * delta vs iteration\n",
    "        * cumulative time vs iteration\n",
    "    \"\"\"\n",
    "    # Delta vs iteration\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    for name, (deltas, _) in results.items():\n",
    "        plt.plot(range(len(deltas)), deltas, label=name)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Max |ΔV|\")\n",
    "    plt.title(title + \" - Convergence (Delta)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"convergence_delta.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Time vs iteration\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    for name, (_, times) in results.items():\n",
    "        plt.plot(range(len(times)), times, label=name)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Cumulative Time (s)\")\n",
    "    plt.title(title + \" - Convergence (Time)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"convergence_time.png\"))\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def sample_and_plot_value_policy(\n",
    "    V_history: List[np.ndarray],\n",
    "    policy_history: List[np.ndarray],\n",
    "    rows: int,\n",
    "    cols: int,\n",
    "    base_title: str,\n",
    "    out_dir: str,\n",
    "    max_samples: int = 4,\n",
    "):\n",
    "    \"\"\"\n",
    "    Sample up to max_samples iterations from histories and plot heatmaps+arrows.\n",
    "    \"\"\"\n",
    "    if len(V_history) == 0:\n",
    "        return\n",
    "\n",
    "    indices = np.linspace(0, len(V_history) - 1, num=min(max_samples, len(V_history)), dtype=int)\n",
    "    for idx in indices:\n",
    "        V = V_history[idx]\n",
    "        policy = policy_history[idx]\n",
    "        iter_title = f\"{base_title} - Iter {idx}\"\n",
    "        heat_path = os.path.join(out_dir, f\"value_iter_{idx}.png\")\n",
    "        pol_path = os.path.join(out_dir, f\"policy_iter_{idx}.png\")\n",
    "        plot_value_heatmap(V, rows, cols, iter_title, heat_path)\n",
    "        plot_policy_arrows(policy, rows, cols, iter_title, pol_path)\n",
    "\n",
    "\n",
    "\n",
    "# FrozenLake Transition Model Extraction\n",
    "def extract_transition_model_from_frozenlake(env) -> Tuple[Dict[int, Dict[int, List[Tuple[float, int, float, bool]]]], int, int]:\n",
    "    \"\"\"\n",
    "    Extract P from FrozenLake-v1 env.unwrapped.P into our DP format.\n",
    "    \"\"\"\n",
    "    envP = env.unwrapped.P\n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n\n",
    "\n",
    "    P = {s: {a: [] for a in range(nA)} for s in range(nS)}\n",
    "    for s, adict in envP.items():\n",
    "        for a, transitions in adict.items():\n",
    "            P[s][a] = transitions  # already (prob, next_state, reward, done)\n",
    "\n",
    "    return P, nS, nA\n",
    "\n",
    "\n",
    "\n",
    "# Experiments\n",
    "def run_gridworld_experiments(out_root: str):\n",
    "    print(\"=== GridWorld Experiments ===\")\n",
    "    ensure_dir(out_root)\n",
    "\n",
    "    # Two configurations: deterministic and stochastic\n",
    "    configs = [\n",
    "        (\"deterministic\",\n",
    "         dict(\n",
    "             shape=(4, 4),\n",
    "             start_state=0,\n",
    "             terminal_states={15: 1.0},\n",
    "             obstacle_states=[5, 7],  # example obstacles\n",
    "             step_cost=-0.04,\n",
    "             stochastic=False,\n",
    "             intended_prob=1.0,\n",
    "         )),\n",
    "        (\"stochastic\",\n",
    "         dict(\n",
    "             shape=(4, 4),\n",
    "             start_state=0,\n",
    "             terminal_states={15: 1.0},\n",
    "             obstacle_states=[5, 7],\n",
    "             step_cost=-0.04,\n",
    "             stochastic=True,\n",
    "             intended_prob=0.8,  # 80% intended, 10% left/right\n",
    "         )),\n",
    "    ]\n",
    "\n",
    "    gamma = 0.99\n",
    "    theta = 1e-6\n",
    "\n",
    "    for name, cfg in configs:\n",
    "        print(f\"\\n--- Config: {name} ---\")\n",
    "        env = GridWorldEnv(**cfg)\n",
    "        rows, cols = cfg[\"shape\"]\n",
    "        P = env.P\n",
    "        nS, nA = env.nS, env.nA\n",
    "\n",
    "        cfg_out_dir = os.path.join(out_root, name)\n",
    "        ensure_dir(cfg_out_dir)\n",
    "\n",
    "        # ---------------- Policy Iteration ----------------\n",
    "        print(\"Running Policy Iteration (in-place eval)...\")\n",
    "        (\n",
    "            V_pi_inplace,\n",
    "            policy_pi_inplace,\n",
    "            V_hist_pi_inplace,\n",
    "            pol_hist_pi_inplace,\n",
    "            deltas_pi_inplace,\n",
    "            times_pi_inplace,\n",
    "        ) = policy_iteration(\n",
    "            P=P,\n",
    "            nS=nS,\n",
    "            nA=nA,\n",
    "            gamma=gamma,\n",
    "            theta=theta,\n",
    "            eval_in_place=True,\n",
    "        )\n",
    "\n",
    "        print(\"Running Policy Iteration (synchronous eval)...\")\n",
    "        (\n",
    "            V_pi_sync,\n",
    "            policy_pi_sync,\n",
    "            V_hist_pi_sync,\n",
    "            pol_hist_pi_sync,\n",
    "            deltas_pi_sync,\n",
    "            times_pi_sync,\n",
    "        ) = policy_iteration(\n",
    "            P=P,\n",
    "            nS=nS,\n",
    "            nA=nA,\n",
    "            gamma=gamma,\n",
    "            theta=theta,\n",
    "            eval_in_place=False,\n",
    "        )\n",
    "\n",
    "        # Sample/plot\n",
    "        pi_inplace_dir = os.path.join(cfg_out_dir, \"policy_iteration_inplace\")\n",
    "        pi_sync_dir = os.path.join(cfg_out_dir, \"policy_iteration_sync\")\n",
    "        ensure_dir(pi_inplace_dir)\n",
    "        ensure_dir(pi_sync_dir)\n",
    "\n",
    "        sample_and_plot_value_policy(\n",
    "            V_hist_pi_inplace,\n",
    "            pol_hist_pi_inplace,\n",
    "            rows,\n",
    "            cols,\n",
    "            base_title=f\"GridWorld {name} - PI In-place\",\n",
    "            out_dir=pi_inplace_dir,\n",
    "        )\n",
    "        sample_and_plot_value_policy(\n",
    "            V_hist_pi_sync,\n",
    "            pol_hist_pi_sync,\n",
    "            rows,\n",
    "            cols,\n",
    "            base_title=f\"GridWorld {name} - PI Sync\",\n",
    "            out_dir=pi_sync_dir,\n",
    "        )\n",
    "\n",
    "        # ---------------- Value Iteration ----------------\n",
    "        print(\"Running Value Iteration (in-place)...\")\n",
    "        (\n",
    "            V_vi_inplace,\n",
    "            policy_vi_inplace,\n",
    "            V_hist_vi_inplace,\n",
    "            pol_hist_vi_inplace,\n",
    "            deltas_vi_inplace,\n",
    "            times_vi_inplace,\n",
    "        ) = value_iteration(\n",
    "            P=P,\n",
    "            nS=nS,\n",
    "            nA=nA,\n",
    "            gamma=gamma,\n",
    "            theta=theta,\n",
    "            in_place=True,\n",
    "        )\n",
    "\n",
    "        print(\"Running Value Iteration (synchronous)...\")\n",
    "        (\n",
    "            V_vi_sync,\n",
    "            policy_vi_sync,\n",
    "            V_hist_vi_sync,\n",
    "            pol_hist_vi_sync,\n",
    "            deltas_vi_sync,\n",
    "            times_vi_sync,\n",
    "        ) = value_iteration(\n",
    "            P=P,\n",
    "            nS=nS,\n",
    "            nA=nA,\n",
    "            gamma=gamma,\n",
    "            theta=theta,\n",
    "            in_place=False,\n",
    "        )\n",
    "\n",
    "        vi_inplace_dir = os.path.join(cfg_out_dir, \"value_iteration_inplace\")\n",
    "        vi_sync_dir = os.path.join(cfg_out_dir, \"value_iteration_sync\")\n",
    "        ensure_dir(vi_inplace_dir)\n",
    "        ensure_dir(vi_sync_dir)\n",
    "\n",
    "        sample_and_plot_value_policy(\n",
    "            V_hist_vi_inplace,\n",
    "            pol_hist_vi_inplace,\n",
    "            rows,\n",
    "            cols,\n",
    "            base_title=f\"GridWorld {name} - VI In-place\",\n",
    "            out_dir=vi_inplace_dir,\n",
    "        )\n",
    "        sample_and_plot_value_policy(\n",
    "            V_hist_vi_sync,\n",
    "            pol_hist_vi_sync,\n",
    "            rows,\n",
    "            cols,\n",
    "            base_title=f\"GridWorld {name} - VI Sync\",\n",
    "            out_dir=vi_sync_dir,\n",
    "        )\n",
    "\n",
    "        # ---------------- Convergence Plots ----------------\n",
    "        conv_dir = os.path.join(cfg_out_dir, \"convergence\")\n",
    "        ensure_dir(conv_dir)\n",
    "        convergence_results = {\n",
    "            \"PI (eval in-place)\": (deltas_pi_inplace, times_pi_inplace),\n",
    "            \"PI (eval sync)\": (deltas_pi_sync, times_pi_sync),\n",
    "            \"VI (in-place)\": (deltas_vi_inplace, times_vi_inplace),\n",
    "            \"VI (sync)\": (deltas_vi_sync, times_vi_sync),\n",
    "        }\n",
    "        plot_convergence_curves(\n",
    "            convergence_results,\n",
    "            title=f\"GridWorld {name}\",\n",
    "            out_dir=conv_dir,\n",
    "        )\n",
    "\n",
    "        # Print a quick summary\n",
    "        print(f\"PI in-place:  final |ΔV|={deltas_pi_inplace[-1]:.2e}, sweeps={len(deltas_pi_inplace)}\")\n",
    "        print(f\"PI sync:      final |ΔV|={deltas_pi_sync[-1]:.2e}, sweeps={len(deltas_pi_sync)}\")\n",
    "        print(f\"VI in-place:  final |ΔV|={deltas_vi_inplace[-1]:.2e}, sweeps={len(deltas_vi_inplace)}\")\n",
    "        print(f\"VI sync:      final |ΔV|={deltas_vi_sync[-1]:.2e}, sweeps={len(deltas_vi_sync)}\")\n",
    "\n",
    "\n",
    "def run_frozenlake_experiments(out_root: str):\n",
    "    print(\"\\n=== FrozenLake-v1 Experiments ===\")\n",
    "    ensure_dir(out_root)\n",
    "\n",
    "    # You can change is_slippery for deterministic vs stochastic tests.\n",
    "    # Here we'll use the default (stochastic).\n",
    "    env = gym.make(\"FrozenLake-v1\", is_slippery=True)\n",
    "    P, nS, nA = extract_transition_model_from_frozenlake(env)\n",
    "\n",
    "    gamma = 0.99\n",
    "    theta = 1e-6\n",
    "\n",
    "    print(\"Running Value Iteration (in-place) on FrozenLake...\")\n",
    "    (\n",
    "        V_vi_inplace,\n",
    "        policy_vi_inplace,\n",
    "        V_hist_vi_inplace,\n",
    "        pol_hist_vi_inplace,\n",
    "        deltas_vi_inplace,\n",
    "        times_vi_inplace,\n",
    "    ) = value_iteration(\n",
    "        P=P,\n",
    "        nS=nS,\n",
    "        nA=nA,\n",
    "        gamma=gamma,\n",
    "        theta=theta,\n",
    "        in_place=True,\n",
    "    )\n",
    "\n",
    "    print(\"Running Policy Iteration (in-place eval) on FrozenLake...\")\n",
    "    (\n",
    "        V_pi_inplace,\n",
    "        policy_pi_inplace,\n",
    "        V_hist_pi_inplace,\n",
    "        pol_hist_pi_inplace,\n",
    "        deltas_pi_inplace,\n",
    "        times_pi_inplace,\n",
    "    ) = policy_iteration(\n",
    "        P=P,\n",
    "        nS=nS,\n",
    "        nA=nA,\n",
    "        gamma=gamma,\n",
    "        theta=theta,\n",
    "        eval_in_place=True,\n",
    "    )\n",
    "\n",
    "    # We can visualize value function and policy only for final iteration (FrozenLake is 4x4 by default)\n",
    "    rows = cols = int(np.sqrt(nS))\n",
    "    final_dir = os.path.join(out_root, \"final\")\n",
    "    ensure_dir(final_dir)\n",
    "\n",
    "    plot_value_heatmap(\n",
    "        V_vi_inplace,\n",
    "        rows,\n",
    "        cols,\n",
    "        title=\"FrozenLake - VI In-place - Final V\",\n",
    "        out_path=os.path.join(final_dir, \"frozenlake_vi_value.png\"),\n",
    "    )\n",
    "    plot_policy_arrows(\n",
    "        policy_vi_inplace,\n",
    "        rows,\n",
    "        cols,\n",
    "        title=\"FrozenLake - VI In-place - Final Policy\",\n",
    "        out_path=os.path.join(final_dir, \"frozenlake_vi_policy.png\"),\n",
    "    )\n",
    "\n",
    "    plot_value_heatmap(\n",
    "        V_pi_inplace,\n",
    "        rows,\n",
    "        cols,\n",
    "        title=\"FrozenLake - PI In-place Eval - Final V\",\n",
    "        out_path=os.path.join(final_dir, \"frozenlake_pi_value.png\"),\n",
    "    )\n",
    "    plot_policy_arrows(\n",
    "        policy_pi_inplace,\n",
    "        rows,\n",
    "        cols,\n",
    "        title=\"FrozenLake - PI In-place Eval - Final Policy\",\n",
    "        out_path=os.path.join(final_dir, \"frozenlake_pi_policy.png\"),\n",
    "    )\n",
    "\n",
    "    # Convergence curves for FrozenLake\n",
    "    conv_dir = os.path.join(out_root, \"convergence\")\n",
    "    ensure_dir(conv_dir)\n",
    "    convergence_results = {\n",
    "        \"FrozenLake VI (in-place)\": (deltas_vi_inplace, times_vi_inplace),\n",
    "        \"FrozenLake PI (eval in-place)\": (deltas_pi_inplace, times_pi_inplace),\n",
    "    }\n",
    "    plot_convergence_curves(\n",
    "        convergence_results,\n",
    "        title=\"FrozenLake-v1\",\n",
    "        out_dir=conv_dir,\n",
    "    )\n",
    "\n",
    "    print(f\"FrozenLake VI: final |ΔV|={deltas_vi_inplace[-1]:.2e}, sweeps={len(deltas_vi_inplace)}\")\n",
    "    print(f\"FrozenLake PI: final |ΔV|={deltas_pi_inplace[-1]:.2e}, sweeps={len(deltas_pi_inplace)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Main\n",
    "def main():\n",
    "    set_seed(42)\n",
    "\n",
    "    OUT_ROOT = \"outputs_dp_lab2\"\n",
    "    ensure_dir(OUT_ROOT)\n",
    "\n",
    "    # Run on custom GridWorld (deterministic + stochastic)\n",
    "    run_gridworld_experiments(os.path.join(OUT_ROOT, \"gridworld\"))\n",
    "\n",
    "    # Run on FrozenLake-v1 using its transition model\n",
    "    run_frozenlake_experiments(os.path.join(OUT_ROOT, \"frozenlake\"))\n",
    "\n",
    "    print(\"\\nAll experiments completed. Check the 'outputs_dp_lab2' folder for plots.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Experiments for MSDS 684 - Week 2 Dynamic Programming\n",
    "These experiments extend dp_lab2.py.\n",
    "\n",
    "Experiments included:\n",
    "1. Stochasticity Sweep (intended_prob sweep)\n",
    "2. Terminal Reward Sensitivity\n",
    "3. Step Cost Variation (Reward Shaping)\n",
    "4. FrozenLake Deterministic vs Stochastic\n",
    "5. PI vs VI Convergence Comparison under noisy transitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA EXPERIMENTS \n",
    "\n",
    "# Helper to run Value Iteration and print key results\n",
    "def run_vi_show(P, nS, nA, gamma=0.99, theta=1e-6):\n",
    "    \"\"\"\n",
    "    Runs Value Iteration and prints:\n",
    "    - number of sweeps\n",
    "    - final delta\n",
    "    - sample of the value function\n",
    "\n",
    "    Used for quick visibility inside the notebook.\n",
    "    \"\"\"\n",
    "    V, policy, V_hist, pol_hist, deltas, times = value_iteration(\n",
    "        P=P, nS=nS, nA=nA, gamma=gamma, theta=theta, in_place=True\n",
    "    )\n",
    "\n",
    "    print(f\"  VI sweeps: {len(deltas)} | final Δ = {deltas[-1]:.2e}\")\n",
    "    print(\"  Sample V:\", V[:4], \"...\")\n",
    "    return deltas, times, V, policy\n",
    "\n",
    "\n",
    "# Helper to run Policy Iteration and print key results\n",
    "def run_pi_show(P, nS, nA, gamma=0.99, theta=1e-6):\n",
    "    \"\"\"\n",
    "    Runs Policy Iteration and prints:\n",
    "    - number of sweeps\n",
    "    - final delta\n",
    "    - sample of the value function\n",
    "    \"\"\"\n",
    "    V, policy, V_hist, pol_hist, deltas, times = policy_iteration(\n",
    "        P=P, nS=nS, nA=nA, gamma=gamma, theta=theta, eval_in_place=True\n",
    "    )\n",
    "\n",
    "    print(f\"  PI sweeps: {len(deltas)} | final Δ = {deltas[-1]:.2e}\")\n",
    "    print(\"  Sample V:\", V[:4], \"...\")\n",
    "    return deltas, times, V, policy\n",
    "\n",
    "\n",
    "\n",
    "# 1. STOCHASTICITY SWEEP\n",
    "def experiment_stochasticity_sweep(outdir=\"outputs_dp_lab2/extra/stochasticity_sweep\"):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        Test how increasing randomness (lower intended_prob) affects\n",
    "        DP convergence for both VI and PI.\n",
    "\n",
    "    What it shows:\n",
    "        - Stochastic environments slow down convergence.\n",
    "        - VI is more sensitive to noise than PI.\n",
    "\n",
    "    Saves:\n",
    "        - Convergence plots (delta/time curves)\n",
    "        - Prints sweeps and sample values\n",
    "    \"\"\"\n",
    "    ensure_dir(outdir)\n",
    "    probs = [1.0, 0.8, 0.6, 0.4, 0.2]\n",
    "\n",
    "    print(\"\\n=== EXTRA: Stochasticity Sweep ===\")\n",
    "    results = {}\n",
    "\n",
    "    for p in probs:\n",
    "        print(f\"\\n--- intended_prob = {p} ---\")\n",
    "\n",
    "        # Create GridWorld with adjustable stochasticity\n",
    "        env = GridWorldEnv(\n",
    "            shape=(4, 4),\n",
    "            start_state=0,\n",
    "            terminal_states={15: 1.0},\n",
    "            obstacle_states=[5, 7],\n",
    "            step_cost=-0.04,\n",
    "            stochastic=True if p < 1.0 else False,\n",
    "            intended_prob=p,\n",
    "        )\n",
    "\n",
    "        P, nS, nA = env.P, env.nS, env.nA\n",
    "\n",
    "        print(\"Value Iteration:\")\n",
    "        del_vi, t_vi, _, _ = run_vi_show(P, nS, nA)\n",
    "\n",
    "        print(\"Policy Iteration:\")\n",
    "        del_pi, t_pi, _, _ = run_pi_show(P, nS, nA)\n",
    "\n",
    "        results[f\"VI p={p}\"] = (del_vi, t_vi)\n",
    "        results[f\"PI p={p}\"] = (del_pi, t_pi)\n",
    "\n",
    "    # Plot convergence curves for all stochastic settings\n",
    "    plot_convergence_curves(results, \"Stochasticity Sweep\", outdir)\n",
    "    print(\"Plots saved to:\", outdir)\n",
    "\n",
    "\n",
    "\n",
    "# 2. TERMINAL REWARD SENSITIVITY\n",
    "def experiment_terminal_reward_sensitivity(outdir=\"outputs_dp_lab2/extra/terminal_rewards\"):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        Show how different terminal rewards (positive/negative)\n",
    "        change value propagation and convergence speed.\n",
    "\n",
    "    What it shows:\n",
    "        - Higher rewards spread faster and create sharper policies.\n",
    "        - Negative rewards flatten the V function.\n",
    "\n",
    "    Saves:\n",
    "        - Convergence curve\n",
    "        - Prints sweeps and example values\n",
    "    \"\"\"\n",
    "    ensure_dir(outdir)\n",
    "    rewards = [1, 2, 5, -1]\n",
    "\n",
    "    print(\"\\n=== EXTRA: Terminal Reward Sensitivity ===\")\n",
    "    results = {}\n",
    "\n",
    "    for r in rewards:\n",
    "        print(f\"\\n--- terminal reward = {r} ---\")\n",
    "\n",
    "        # Create GridWorld with modified terminal reward\n",
    "        env = GridWorldEnv(\n",
    "            shape=(4, 4),\n",
    "            start_state=0,\n",
    "            terminal_states={15: r},\n",
    "            obstacle_states=[5, 7],\n",
    "            step_cost=-0.04,\n",
    "            stochastic=False,\n",
    "        )\n",
    "\n",
    "        P, nS, nA = env.P, env.nS, env.nA\n",
    "        del_vi, t_vi, _, _ = run_vi_show(P, nS, nA)\n",
    "\n",
    "        results[f\"terminal={r}\"] = (del_vi, t_vi)\n",
    "\n",
    "    plot_convergence_curves(results, \"Terminal Reward Sensitivity\", outdir)\n",
    "    print(\"Plots saved to:\", outdir)\n",
    "\n",
    "\n",
    "# 3. STEP-COST VARIATION (REWARD SHAPING)\n",
    "def experiment_step_cost_variation(outdir=\"outputs_dp_lab2/extra/step_cost\"):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        Test how movement penalties affect policy preference and convergence.\n",
    "\n",
    "    What it shows:\n",
    "        - No movement cost → flat, slow values.\n",
    "        - Small penalty → encourages shorter paths.\n",
    "        - Larger penalty → strongly pushes agent toward termination.\n",
    "\n",
    "    Saves:\n",
    "        - Convergence curves\n",
    "        - Prints sweeps + example V values\n",
    "    \"\"\"\n",
    "    ensure_dir(outdir)\n",
    "    step_costs = [0.0, -0.01, -0.1]\n",
    "    results = {}\n",
    "\n",
    "    print(\"\\n=== EXTRA: Step-Cost Variation ===\")\n",
    "\n",
    "    for sc in step_costs:\n",
    "        print(f\"\\n--- step_cost = {sc} ---\")\n",
    "\n",
    "        env = GridWorldEnv(\n",
    "            shape=(4, 4),\n",
    "            start_state=0,\n",
    "            terminal_states={15: 1.0},\n",
    "            obstacle_states=[5, 7],\n",
    "            step_cost=sc,\n",
    "            stochastic=False,\n",
    "        )\n",
    "\n",
    "        P, nS, nA = env.P, env.nS, env.nA\n",
    "        del_vi, t_vi, _, _ = run_vi_show(P, nS, nA)\n",
    "\n",
    "        results[f\"step_cost={sc}\"] = (del_vi, t_vi)\n",
    "\n",
    "    plot_convergence_curves(results, \"Step Cost Variation\", outdir)\n",
    "    print(\"Plots saved to:\", outdir)\n",
    "\n",
    "\n",
    "# 4. FROZENLAKE: Deterministic vs Stochastic\n",
    "def experiment_frozenlake_compare(outdir=\"outputs_dp_lab2/extra/frozenlake_compare\"):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "        Compare how VI behaves on deterministic vs slippery FrozenLake.\n",
    "\n",
    "    What it shows:\n",
    "        - Deterministic converges fast.\n",
    "        - Slippery version requires more sweeps due to uncertainty.\n",
    "\n",
    "    Saves:\n",
    "        - Convergence curves\n",
    "        - Prints sweeps + example values\n",
    "    \"\"\"\n",
    "    ensure_dir(outdir)\n",
    "    configs = [(\"deterministic\", False), (\"stochastic\", True)]\n",
    "    results = {}\n",
    "\n",
    "    print(\"\\n=== EXTRA: FrozenLake Comparison ===\")\n",
    "\n",
    "    for name, slip in configs:\n",
    "        print(f\"\\n--- FrozenLake is_slippery = {slip} ---\")\n",
    "\n",
    "        env = gym.make(\"FrozenLake-v1\", is_slippery=slip)\n",
    "        P, nS, nA = extract_transition_model_from_frozenlake(env)\n",
    "\n",
    "        del_vi, t_vi, _, _ = run_vi_show(P, nS, nA)\n",
    "\n",
    "        results[name] = (del_vi, t_vi)\n",
    "\n",
    "    plot_convergence_curves(results, \"FrozenLake Comparison\", outdir)\n",
    "    print(\"Plots saved to:\", outdir)\n",
    "\n",
    "\n",
    "# RUN ALL EXTRA EXPERIMENTS\n",
    "def run_all_extra_experiments():\n",
    "    \"\"\"\n",
    "    Runs:\n",
    "        - Stochasticity Sweep\n",
    "        - Terminal Reward Sensitivity\n",
    "        - Step Cost Variation\n",
    "        - FrozenLake Comparison\n",
    "\n",
    "    This gives a complete extension of the lab requirements.\n",
    "    \"\"\"\n",
    "    base = \"outputs_dp_lab2/extra\"\n",
    "    ensure_dir(base)\n",
    "\n",
    "    experiment_stochasticity_sweep()\n",
    "    experiment_terminal_reward_sensitivity()\n",
    "    experiment_step_cost_variation()\n",
    "    experiment_frozenlake_compare()\n",
    "\n",
    "    print(\"\\n=== All EXTRA experiments completed! ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXTRA: Stochasticity Sweep ===\n",
      "\n",
      "--- intended_prob = 1.0 ---\n",
      "Value Iteration:\n",
      "  VI sweeps: 7 | final Δ = 0.00e+00\n",
      "  Sample V: [0.71691065 0.76455621 0.81268304 0.76455621] ...\n",
      "Policy Iteration:\n",
      "  PI sweeps: 298 | final Δ = 0.00e+00\n",
      "  Sample V: [0.71691065 0.76455621 0.81268304 0.76455621] ...\n",
      "\n",
      "--- intended_prob = 0.8 ---\n",
      "Value Iteration:\n",
      "  VI sweeps: 20 | final Δ = 5.90e-07\n",
      "  Sample V: [0.64872865 0.68458726 0.74373635 0.68458747] ...\n",
      "Policy Iteration:\n",
      "  PI sweeps: 324 | final Δ = 5.94e-07\n",
      "  Sample V: [0.64872865 0.68458725 0.74373635 0.68458747] ...\n",
      "\n",
      "--- intended_prob = 0.6 ---\n",
      "Value Iteration:\n",
      "  VI sweeps: 36 | final Δ = 8.32e-07\n",
      "  Sample V: [0.52210561 0.53976047 0.6161883  0.53976099] ...\n",
      "Policy Iteration:\n",
      "  PI sweeps: 357 | final Δ = 8.41e-07\n",
      "  Sample V: [0.5221056  0.53976045 0.61618829 0.53976098] ...\n",
      "\n",
      "--- intended_prob = 0.4 ---\n",
      "Value Iteration:\n",
      "  VI sweeps: 62 | final Δ = 9.15e-07\n",
      "  Sample V: [0.26876594 0.23232813 0.33839442 0.23153607] ...\n",
      "Policy Iteration:\n",
      "  PI sweeps: 493 | final Δ = 8.02e-07\n",
      "  Sample V: [0.26876613 0.23232843 0.33839466 0.23153648] ...\n",
      "\n",
      "--- intended_prob = 0.2 ---\n",
      "Value Iteration:\n",
      "  VI sweeps: 95 | final Δ = 9.39e-07\n",
      "  Sample V: [0.0543638  0.0919458  0.23286078 0.12860206] ...\n",
      "Policy Iteration:\n",
      "  PI sweeps: 479 | final Δ = 9.88e-07\n",
      "  Sample V: [0.05436344 0.09194548 0.23286055 0.12860179] ...\n",
      "Plots saved to: outputs_dp_lab2/extra/stochasticity_sweep\n",
      "\n",
      "=== EXTRA: Terminal Reward Sensitivity ===\n",
      "\n",
      "--- terminal reward = 1 ---\n",
      "  VI sweeps: 7 | final Δ = 0.00e+00\n",
      "  Sample V: [0.71691065 0.76455621 0.81268304 0.76455621] ...\n",
      "\n",
      "--- terminal reward = 2 ---\n",
      "  VI sweeps: 7 | final Δ = 0.00e+00\n",
      "  Sample V: [1.6679007  1.72515222 1.78298204 1.72515222] ...\n",
      "\n",
      "--- terminal reward = 5 ---\n",
      "  VI sweeps: 7 | final Δ = 0.00e+00\n",
      "  Sample V: [4.52087085 4.60694025 4.69387904 4.60694025] ...\n",
      "\n",
      "--- terminal reward = -1 ---\n",
      "  VI sweeps: 36 | final Δ = 0.00e+00\n",
      "  Sample V: [-1.18506945 -1.15663581 -1.12791496 -1.15663581] ...\n",
      "Plots saved to: outputs_dp_lab2/extra/terminal_rewards\n",
      "\n",
      "=== EXTRA: Step-Cost Variation ===\n",
      "\n",
      "--- step_cost = 0.0 ---\n",
      "  VI sweeps: 7 | final Δ = 0.00e+00\n",
      "  Sample V: [0.95099005 0.96059601 0.970299   0.96059601] ...\n",
      "\n",
      "--- step_cost = -0.01 ---\n",
      "  VI sweeps: 7 | final Δ = 0.00e+00\n",
      "  Sample V: [0.8924702  0.91158606 0.93089501 0.91158606] ...\n",
      "\n",
      "--- step_cost = -0.1 ---\n",
      "  VI sweeps: 7 | final Δ = 0.00e+00\n",
      "  Sample V: [0.36579154 0.47049651 0.5762591  0.47049651] ...\n",
      "Plots saved to: outputs_dp_lab2/extra/step_cost\n",
      "\n",
      "=== EXTRA: FrozenLake Comparison ===\n",
      "\n",
      "--- FrozenLake is_slippery = False ---\n",
      "  VI sweeps: 7 | final Δ = 0.00e+00\n",
      "  Sample V: [0.95099005 0.96059601 0.970299   0.96059601] ...\n",
      "\n",
      "--- FrozenLake is_slippery = True ---\n",
      "  VI sweeps: 228 | final Δ = 9.72e-07\n",
      "  Sample V: [0.54201404 0.49878743 0.47067727 0.45683193] ...\n",
      "Plots saved to: outputs_dp_lab2/extra/frozenlake_compare\n",
      "\n",
      "=== All EXTRA experiments completed! ===\n"
     ]
    }
   ],
   "source": [
    "run_all_extra_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Experiments – Summary and Analysis\n",
    "\n",
    "To better understand how Dynamic Programming behaves under different conditions, I ran a set of extra experiments that extend the core lab requirements. First, I tested how increasing stochasticity affects convergence by gradually lowering the intended action probability. As the environment became more random, both Policy Iteration and Value Iteration converged more slowly, which makes sense because uncertainty makes value propagation less clear and Q-value differences shrink. I also experimented with different terminal rewards (+1, +2, +5, –1) to see how sensitive DP is to reward changes. Larger rewards pushed values higher across the grid and led to faster convergence, while negative terminal rewards flattened the value function and encouraged the agent to avoid the goal. I then varied the step cost (0, –0.01, –0.1) to observe how reward shaping affects the optimal behavior. Small negative step costs helped values propagate more cleanly and encouraged shorter paths, while larger penalties made the agent strongly prefer quick termination. Finally, I compared deterministic and stochastic versions of FrozenLake, where deterministic transitions converged quickly and the slippery version required more sweeps due to unpredictable outcomes. Overall, these extra tests showed that DP algorithms are highly sensitive to transition randomness and reward structure, which directly impacts convergence speed, policy stability, and the clarity of the optimal path."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
