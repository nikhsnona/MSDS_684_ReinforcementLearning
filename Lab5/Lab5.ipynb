{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MSDS 684 - Lab 5\n",
    "Semi-gradient SARSA with Tile Coding on MountainCar-v0\n",
    "\n",
    "This script:\n",
    "- Implements tile coding from scratch using NumPy (configurable tilings/tiles/offsets)\n",
    "- Solves MountainCar-v0 using semi-gradient SARSA with linear function approximation\n",
    "- Tracks steps per episode (learning curve)\n",
    "- Saves numeric data for each graph so you can share results\n",
    "- Prints numeric summaries to the console\n",
    "- Generates:\n",
    "    1. Learning curve (steps per episode)\n",
    "    2. Value function heatmap: max_a Q(s, a) over (position, velocity)\n",
    "    3. Greedy policy map across the state space\n",
    "    4. Sample greedy trajectories overlayed on the value function\n",
    "    5. Convergence curves for different feature configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training baseline config: 8 tilings, 8x8 tiles ===\n",
      "Episode 50/500 - steps: 200, epsilon: 0.778\n",
      "Episode 100/500 - steps: 200, epsilon: 0.606\n",
      "Episode 150/500 - steps: 200, epsilon: 0.471\n",
      "Episode 200/500 - steps: 200, epsilon: 0.367\n",
      "Episode 250/500 - steps: 188, epsilon: 0.286\n",
      "Episode 300/500 - steps: 161, epsilon: 0.222\n",
      "Episode 350/500 - steps: 161, epsilon: 0.173\n",
      "Episode 400/500 - steps: 159, epsilon: 0.135\n",
      "Episode 450/500 - steps: 200, epsilon: 0.105\n",
      "Episode 500/500 - steps: 117, epsilon: 0.082\n",
      "\n",
      "=== Learning Curve (Steps per Episode) ===\n",
      "Total episodes: 500\n",
      "First 10 episodes: [200, 200, 200, 200, 200, 200, 200, 200, 200, 200]\n",
      "Last 10 episodes:  [114, 153, 109, 171, 151, 178, 119, 151, 169, 117]\n",
      "Mean steps over last 50 episodes: 136.20\n",
      "\n",
      "=== Value Function Grid (max_a Q(s,a)) ===\n",
      "Grid shape: (60, 60) (vel x pos)\n",
      "Min value: -96.9105\n",
      "Max value: 0.4314\n",
      "Middle velocity row (first 10 positions): [-34.938, -34.938, -36.475, -39.42, -41.876, -45.372, -48.04, -49.786, -51.098, -52.375]\n",
      "\n",
      "=== Greedy Policy Grid ===\n",
      "Action 0: 1524 cells (42.3%)\n",
      "Action 1: 415 cells (11.5%)\n",
      "Action 2: 1661 cells (46.1%)\n",
      "\n",
      "=== Greedy Trajectories ===\n",
      "Trajectory 1: 156 steps\n",
      "Trajectory 2: 84 steps\n",
      "Trajectory 3: 92 steps\n",
      "Trajectory 4: 158 steps\n",
      "Trajectory 5: 145 steps\n",
      "\n",
      "=== Running config: 4 tilings, 4x4 ===\n",
      "\n",
      "=== Running config: 8 tilings, 8x8 ===\n",
      "\n",
      "=== Running config: 16 tilings, 8x8 ===\n",
      "\n",
      "=== Convergence Summary (steps per episode) ===\n",
      "4 tilings, 4x4: final episode=146, mean of last 50 episodes=188.24\n",
      "8 tilings, 8x8: final episode=184, mean of last 50 episodes=179.26\n",
      "16 tilings, 8x8: final episode=193, mean of last 50 episodes=181.70\n",
      "Convergence plot + data saved to lab5_outputs\\convergence_feature_configs.png, lab5_outputs\\data\\convergence_feature_configs.csv\n",
      "\n",
      "Training complete. All figures in: C:\\Users\\nikhs\\Downloads\\Reinforcement Learning\\Lab5\\lab5_outputs\n",
      "Numeric data for graphs in: C:\\Users\\nikhs\\Downloads\\Reinforcement Learning\\Lab5\\lab5_outputs\\data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "\n",
    "# plt.ioff()  # turn off interactive mode so plt.show() doesn't interfere with saving\n",
    "\n",
    "# Tile Coding Implementation\n",
    "class TileCoder:\n",
    "    \"\"\"\n",
    "    Uniform tile coder for 2D continuous observations (position, velocity).\n",
    "\n",
    "    - num_tilings: number of overlapping tilings\n",
    "    - tiles_per_dim: number of tiles per dimension per tiling\n",
    "\n",
    "    Features:\n",
    "    - Discretizes the continuous observation space into overlapping grids\n",
    "    - For each tiling, computes which tile the state falls into\n",
    "    - Returns either:\n",
    "        * indices of active tiles, or\n",
    "        * a sparse binary feature vector via get_feature_vector()\n",
    "    \"\"\"\n",
    "    def __init__(self, obs_low, obs_high, num_tilings=8, tiles_per_dim=8):\n",
    "        self.obs_low = np.array(obs_low, dtype=np.float64)\n",
    "        self.obs_high = np.array(obs_high, dtype=np.float64)\n",
    "        self.num_dims = len(self.obs_low)\n",
    "\n",
    "        assert self.num_dims == 2, \"TileCoder is designed for 2D states (position, velocity).\"\n",
    "\n",
    "        self.num_tilings = num_tilings\n",
    "        self.tiles_per_dim = tiles_per_dim\n",
    "\n",
    "        # Width of each tile in each dimension\n",
    "        # (tiles_per_dim - 1) so that edges align with obs_high\n",
    "        self.tile_width = (self.obs_high - self.obs_low) / (self.tiles_per_dim - 1)\n",
    "\n",
    "        # Offsets for each tiling to create overlapping grids\n",
    "        offsets = []\n",
    "        for tiling_idx in range(self.num_tilings):\n",
    "            frac = tiling_idx / self.num_tilings  # goes from 0 to (num_tilings-1)/num_tilings\n",
    "            offset = frac * self.tile_width\n",
    "            offsets.append(offset)\n",
    "        self.offsets = np.array(offsets)\n",
    "\n",
    "        # Total number of tiles per tiling and overall feature dimension\n",
    "        self.tiles_per_tiling = (self.tiles_per_dim ** self.num_dims)\n",
    "        self.n_features = self.num_tilings * self.tiles_per_tiling\n",
    "\n",
    "    def get_tile_indices(self, state):\n",
    "        \"\"\"\n",
    "        Given a continuous state (position, velocity), return a 1D NumPy array\n",
    "        of active tile indices across all tilings (one per tiling).\n",
    "        \"\"\"\n",
    "        state = np.array(state, dtype=np.float64)\n",
    "        state = np.clip(state, self.obs_low, self.obs_high)\n",
    "\n",
    "        active_indices = []\n",
    "\n",
    "        for tiling_idx in range(self.num_tilings):\n",
    "            offset = self.offsets[tiling_idx]\n",
    "\n",
    "            # Shift state by offset and scale by tile width to get tile coordinates\n",
    "            coords = ((state - self.obs_low + offset) / self.tile_width).astype(int)\n",
    "\n",
    "            # Clamp to [0, tiles_per_dim-1]\n",
    "            coords = np.clip(coords, 0, self.tiles_per_dim - 1)\n",
    "\n",
    "            # Flatten (i, j) -> index within this tiling\n",
    "            tile_index_within_tiling = np.ravel_multi_index(\n",
    "                coords,\n",
    "                dims=(self.tiles_per_dim, self.tiles_per_dim)\n",
    "            )\n",
    "\n",
    "            # Global index across all tilings\n",
    "            global_index = tiling_idx * self.tiles_per_tiling + tile_index_within_tiling\n",
    "            active_indices.append(global_index)\n",
    "\n",
    "        return np.array(active_indices, dtype=int)\n",
    "\n",
    "    def get_feature_vector(self, state):\n",
    "        \"\"\"\n",
    "        Return a sparse binary feature vector for the given state.\n",
    "        Mostly for completeness; we usually just work with indices.\n",
    "        \"\"\"\n",
    "        indices = self.get_tile_indices(state)\n",
    "        x = np.zeros(self.n_features, dtype=np.float64)\n",
    "        x[indices] = 1.0\n",
    "        return x\n",
    "\n",
    "\n",
    "# Semi-gradient SARSA Agent\n",
    "class SemiGradientSarsaAgent:\n",
    "    \"\"\"\n",
    "    Semi-gradient SARSA agent with linear function approximation:\n",
    "\n",
    "        Q(s, a) = w_a^T x(s)\n",
    "\n",
    "    where:\n",
    "    - x(s) is a sparse binary feature vector from tile coding\n",
    "    - w_a is a separate weight vector per action\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        tilecoder: TileCoder,\n",
    "        alpha=0.1,\n",
    "        gamma=1.0,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_min=0.01,\n",
    "        epsilon_decay=0.995,\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.tc = tilecoder\n",
    "\n",
    "        self.n_actions = env.action_space.n\n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Scale alpha by number of tilings (common heuristic for tile coding)\n",
    "        self.alpha = alpha / self.tc.num_tilings\n",
    "\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # Weight matrix: one weight vector per action\n",
    "        self.w = np.zeros((self.n_actions, self.tc.n_features), dtype=np.float64)\n",
    "\n",
    "    # ---------- Q-value helpers ----------\n",
    "\n",
    "    def q_value(self, state, action):\n",
    "        indices = self.tc.get_tile_indices(state)\n",
    "        return np.sum(self.w[action, indices])\n",
    "\n",
    "    def q_values(self, state):\n",
    "        indices = self.tc.get_tile_indices(state)\n",
    "        return np.array([np.sum(self.w[a, indices]) for a in range(self.n_actions)])\n",
    "\n",
    "    # ---------- Policy ----------\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        q_vals = self.q_values(state)\n",
    "        return int(np.argmax(q_vals))\n",
    "\n",
    "    def greedy_action(self, state):\n",
    "        q_vals = self.q_values(state)\n",
    "        return int(np.argmax(q_vals))\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "    # ---------- Training ----------\n",
    "\n",
    "    def train(self, num_episodes=500, max_steps_per_episode=200, verbose=True):\n",
    "        \"\"\"\n",
    "        Train with semi-gradient SARSA.\n",
    "\n",
    "        Returns:\n",
    "            episode_lengths: list of steps taken per episode\n",
    "        \"\"\"\n",
    "        episode_lengths = []\n",
    "\n",
    "        for ep in range(num_episodes):\n",
    "            state, _ = self.env.reset()\n",
    "            action = self.select_action(state)\n",
    "\n",
    "            steps = 0\n",
    "            done = False\n",
    "\n",
    "            while not done and steps < max_steps_per_episode:\n",
    "                indices = self.tc.get_tile_indices(state)\n",
    "                q_sa = np.sum(self.w[action, indices])\n",
    "\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "                done = terminated or truncated\n",
    "                steps += 1\n",
    "\n",
    "                if done:\n",
    "                    target = reward\n",
    "                    td_error = target - q_sa\n",
    "                else:\n",
    "                    next_action = self.select_action(next_state)\n",
    "                    next_indices = self.tc.get_tile_indices(next_state)\n",
    "                    q_snext_anext = np.sum(self.w[next_action, next_indices])\n",
    "                    target = reward + self.gamma * q_snext_anext\n",
    "                    td_error = target - q_sa\n",
    "\n",
    "                # Semi-gradient update only on active features\n",
    "                self.w[action, indices] += self.alpha * td_error\n",
    "\n",
    "                if not done:\n",
    "                    state = next_state\n",
    "                    action = next_action\n",
    "\n",
    "            self.decay_epsilon()\n",
    "            episode_lengths.append(steps)\n",
    "\n",
    "            if verbose and (ep + 1) % 50 == 0:\n",
    "                print(\n",
    "                    f\"Episode {ep + 1}/{num_episodes} - \"\n",
    "                    f\"steps: {steps}, epsilon: {self.epsilon:.3f}\"\n",
    "                )\n",
    "\n",
    "        return episode_lengths\n",
    "\n",
    "    # ---------- Greedy rollout for visualization ----------\n",
    "\n",
    "    def run_greedy_episode(self, max_steps=200, start_state=None):\n",
    "        \"\"\"\n",
    "        Run an episode with greedy (exploit-only) policy and optionally set a custom start state.\n",
    "        Returns:\n",
    "            traj_states: array of visited states\n",
    "            steps: number of steps taken\n",
    "        \"\"\"\n",
    "        state, _ = self.env.reset()\n",
    "        if start_state is not None:\n",
    "            # Directly manipulate underlying state\n",
    "            self.env.unwrapped.state = np.array(start_state, dtype=np.float64)\n",
    "            state = np.array(start_state, dtype=np.float64)\n",
    "\n",
    "        done = False\n",
    "        steps = 0\n",
    "        traj_states = [state.copy()]\n",
    "\n",
    "        while not done and steps < max_steps:\n",
    "            action = self.greedy_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "            done = terminated or truncated\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "            traj_states.append(state.copy())\n",
    "\n",
    "        return np.array(traj_states), steps\n",
    "\n",
    "\n",
    "# Visualization + Data Saving Utilities\n",
    "def plot_learning_curve(episode_lengths, out_path_fig, out_path_csv):\n",
    "    \"\"\"\n",
    "    Plot and save the learning curve.\n",
    "    Also saves raw episode lengths to CSV and prints summary statistics.\n",
    "    \"\"\"\n",
    "    episode_lengths = np.array(episode_lengths, dtype=np.int32)\n",
    "\n",
    "    # Save raw numbers\n",
    "    header = \"steps_per_episode\"\n",
    "    np.savetxt(out_path_csv, episode_lengths, delimiter=\",\", header=header, comments=\"\")\n",
    "\n",
    "    # Print numeric summary\n",
    "    print(\"\\n=== Learning Curve (Steps per Episode) ===\")\n",
    "    print(f\"Total episodes: {len(episode_lengths)}\")\n",
    "    print(f\"First 10 episodes: {episode_lengths[:10].tolist()}\")\n",
    "    print(f\"Last 10 episodes:  {episode_lengths[-10:].tolist()}\")\n",
    "    if len(episode_lengths) >= 50:\n",
    "        last50_mean = episode_lengths[-50:].mean()\n",
    "        print(f\"Mean steps over last 50 episodes: {last50_mean:.2f}\")\n",
    "    else:\n",
    "        print(\"Not enough episodes to compute last-50 mean.\")\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(episode_lengths)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Steps per Episode\")\n",
    "    plt.title(\"Learning Curve: MountainCar-v0 (Semi-gradient SARSA + Tile Coding)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(out_path_fig)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_value_function(agent, tc, out_path_fig, out_path_npz, num_points_per_dim=60):\n",
    "    \"\"\"\n",
    "    Compute value grid V(s) = max_a Q(s,a), save numeric data, and plot heatmap.\n",
    "    Also prints basic stats for the grid.\n",
    "    \"\"\"\n",
    "    pos_vals = np.linspace(tc.obs_low[0], tc.obs_high[0], num_points_per_dim)\n",
    "    vel_vals = np.linspace(tc.obs_low[1], tc.obs_high[1], num_points_per_dim)\n",
    "\n",
    "    value_grid = np.zeros((num_points_per_dim, num_points_per_dim), dtype=np.float64)\n",
    "\n",
    "    for i, p in enumerate(pos_vals):\n",
    "        for j, v in enumerate(vel_vals):\n",
    "            state = np.array([p, v], dtype=np.float64)\n",
    "            q_vals = agent.q_values(state)\n",
    "            value_grid[j, i] = np.max(q_vals)  # (j, i) for imshow (y, x)\n",
    "\n",
    "    # Save numeric data\n",
    "    np.savez(out_path_npz, pos=pos_vals, vel=vel_vals, value_grid=value_grid)\n",
    "\n",
    "    # Print numeric summary\n",
    "    print(\"\\n=== Value Function Grid (max_a Q(s,a)) ===\")\n",
    "    print(f\"Grid shape: {value_grid.shape} (vel x pos)\")\n",
    "    print(f\"Min value: {value_grid.min():.4f}\")\n",
    "    print(f\"Max value: {value_grid.max():.4f}\")\n",
    "    mid_row = value_grid[value_grid.shape[0] // 2]\n",
    "    print(f\"Middle velocity row (first 10 positions): {mid_row[:10].round(3).tolist()}\")\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    im = plt.imshow(\n",
    "        value_grid,\n",
    "        origin=\"lower\",\n",
    "        aspect=\"auto\",\n",
    "        extent=[\n",
    "            tc.obs_low[0],\n",
    "            tc.obs_high[0],\n",
    "            tc.obs_low[1],\n",
    "            tc.obs_high[1],\n",
    "        ],\n",
    "    )\n",
    "    plt.colorbar(im, label=\"max_a Q(s, a)\")\n",
    "    plt.xlabel(\"Position\")\n",
    "    plt.ylabel(\"Velocity\")\n",
    "    plt.title(\"Approximate State-Value Function: max_a Q(s, a)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(out_path_fig)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_policy(agent, tc, out_path_fig, out_path_npy, num_points_per_dim=60):\n",
    "    \"\"\"\n",
    "    Compute greedy policy map over state space, save action grid, and plot.\n",
    "    Also prints how many cells choose each action.\n",
    "    \"\"\"\n",
    "    pos_vals = np.linspace(tc.obs_low[0], tc.obs_high[0], num_points_per_dim)\n",
    "    vel_vals = np.linspace(tc.obs_low[1], tc.obs_high[1], num_points_per_dim)\n",
    "\n",
    "    policy_grid = np.zeros((num_points_per_dim, num_points_per_dim), dtype=int)\n",
    "\n",
    "    for i, p in enumerate(pos_vals):\n",
    "        for j, v in enumerate(vel_vals):\n",
    "            state = np.array([p, v], dtype=np.float64)\n",
    "            action = agent.greedy_action(state)\n",
    "            policy_grid[j, i] = action  # (j, i) for imshow\n",
    "\n",
    "    # Save numeric policy grid\n",
    "    np.save(out_path_npy, policy_grid)\n",
    "\n",
    "    # Print numeric summary\n",
    "    print(\"\\n=== Greedy Policy Grid ===\")\n",
    "    unique, counts = np.unique(policy_grid, return_counts=True)\n",
    "    total_cells = policy_grid.size\n",
    "    for a, c in zip(unique, counts):\n",
    "        pct = 100.0 * c / total_cells\n",
    "        print(f\"Action {a}: {c} cells ({pct:.1f}%)\")\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    im = plt.imshow(\n",
    "        policy_grid,\n",
    "        origin=\"lower\",\n",
    "        aspect=\"auto\",\n",
    "        extent=[\n",
    "            tc.obs_low[0],\n",
    "            tc.obs_high[0],\n",
    "            tc.obs_low[1],\n",
    "            tc.obs_high[1],\n",
    "        ],\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "    cbar = plt.colorbar(im, ticks=[0, 1, 2])\n",
    "    cbar.ax.set_yticklabels([\"Left (0)\", \"No push (1)\", \"Right (2)\"])\n",
    "    plt.xlabel(\"Position\")\n",
    "    plt.ylabel(\"Velocity\")\n",
    "    plt.title(\"Greedy Policy: argmax_a Q(s, a)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(out_path_fig)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_trajectories_over_value(\n",
    "    agent,\n",
    "    tc,\n",
    "    out_path_fig,\n",
    "    out_path_npz,\n",
    "    num_traj=5,\n",
    "    max_steps=200,\n",
    "    num_points_per_dim=60,\n",
    "):\n",
    "    \"\"\"\n",
    "    Overlay several greedy trajectories and save numeric data.\n",
    "    Also prints length of each trajectory.\n",
    "    \"\"\"\n",
    "    pos_vals = np.linspace(tc.obs_low[0], tc.obs_high[0], num_points_per_dim)\n",
    "    vel_vals = np.linspace(tc.obs_low[1], tc.obs_high[1], num_points_per_dim)\n",
    "\n",
    "    value_grid = np.zeros((num_points_per_dim, num_points_per_dim), dtype=np.float64)\n",
    "    for i, p in enumerate(pos_vals):\n",
    "        for j, v in enumerate(vel_vals):\n",
    "            state = np.array([p, v], dtype=np.float64)\n",
    "            q_vals = agent.q_values(state)\n",
    "            value_grid[j, i] = np.max(q_vals)\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    im = plt.imshow(\n",
    "        value_grid,\n",
    "        origin=\"lower\",\n",
    "        aspect=\"auto\",\n",
    "        extent=[\n",
    "            tc.obs_low[0],\n",
    "            tc.obs_high[0],\n",
    "            tc.obs_low[1],\n",
    "            tc.obs_high[1],\n",
    "        ],\n",
    "    )\n",
    "    plt.colorbar(im, label=\"max_a Q(s, a)\")\n",
    "\n",
    "    rng = np.random.RandomState(42)\n",
    "    trajectories = []\n",
    "    lengths = []\n",
    "\n",
    "    for k in range(num_traj):\n",
    "        start_pos = rng.uniform(-0.6, -0.4)\n",
    "        start_vel = 0.0\n",
    "        traj_states, steps = agent.run_greedy_episode(\n",
    "            max_steps=max_steps,\n",
    "            start_state=[start_pos, start_vel],\n",
    "        )\n",
    "        trajectories.append(traj_states)\n",
    "        lengths.append(steps)\n",
    "        plt.plot(traj_states[:, 0], traj_states[:, 1], linewidth=1.5)\n",
    "\n",
    "    # Save numeric data (grid + trajectories)\n",
    "    np.savez(\n",
    "        out_path_npz,\n",
    "        pos=pos_vals,\n",
    "        vel=vel_vals,\n",
    "        value_grid=value_grid,\n",
    "        trajectories=np.array(trajectories, dtype=object),\n",
    "    )\n",
    "\n",
    "    # Print numeric summary\n",
    "    print(\"\\n=== Greedy Trajectories ===\")\n",
    "    for i, L in enumerate(lengths, start=1):\n",
    "        print(f\"Trajectory {i}: {L} steps\")\n",
    "\n",
    "    plt.xlabel(\"Position\")\n",
    "    plt.ylabel(\"Velocity\")\n",
    "    plt.title(\"Sample Greedy Trajectories over Value Function\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(out_path_fig)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# Convergence Curves for Different Feature Configurations\n",
    "def experiment_feature_configs(\n",
    "    env_name,\n",
    "    configs,\n",
    "    num_episodes=300,\n",
    "    max_steps=200,\n",
    "    out_path_fig=\"convergence_feature_configs.png\",\n",
    "    out_path_csv=\"convergence_feature_configs.csv\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Run semi-gradient SARSA with different tile coding configs.\n",
    "    Save convergence curves and print summary stats.\n",
    "    \"\"\"\n",
    "    all_curves = []\n",
    "\n",
    "    for cfg in configs:\n",
    "        print(f\"\\n=== Running config: {cfg['label']} ===\")\n",
    "\n",
    "        env = gym.make(env_name)\n",
    "\n",
    "        tilecoder = TileCoder(\n",
    "            obs_low=env.observation_space.low,\n",
    "            obs_high=env.observation_space.high,\n",
    "            num_tilings=cfg[\"num_tilings\"],\n",
    "            tiles_per_dim=cfg[\"tiles_per_dim\"],\n",
    "        )\n",
    "\n",
    "        agent = SemiGradientSarsaAgent(\n",
    "            env=env,\n",
    "            tilecoder=tilecoder,\n",
    "            alpha=0.1,\n",
    "            gamma=1.0,\n",
    "            epsilon_start=1.0,\n",
    "            epsilon_min=0.01,\n",
    "            epsilon_decay=0.995,\n",
    "        )\n",
    "\n",
    "        ep_lengths = agent.train(\n",
    "            num_episodes=num_episodes,\n",
    "            max_steps_per_episode=max_steps,\n",
    "            verbose=False,\n",
    "        )\n",
    "        curve = np.array(ep_lengths, dtype=np.int32)\n",
    "        all_curves.append((cfg[\"label\"], curve))\n",
    "\n",
    "        env.close()\n",
    "\n",
    "    # Save to CSV\n",
    "    max_len = max(len(c[1]) for c in all_curves)\n",
    "    data = np.full((max_len, len(all_curves) + 1), np.nan)\n",
    "    data[:, 0] = np.arange(1, max_len + 1)\n",
    "    header_cols = [\"episode\"]\n",
    "\n",
    "    for j, (label, curve) in enumerate(all_curves, start=1):\n",
    "        data[: len(curve), j] = curve\n",
    "        header_cols.append(label.replace(\",\", \"_\"))\n",
    "\n",
    "    header = \",\".join(header_cols)\n",
    "    np.savetxt(out_path_csv, data, delimiter=\",\", header=header, comments=\"\")\n",
    "\n",
    "    # Print numeric summary\n",
    "    print(\"\\n=== Convergence Summary (steps per episode) ===\")\n",
    "    for label, curve in all_curves:\n",
    "        last50 = curve[-50:] if len(curve) >= 50 else curve\n",
    "        print(\n",
    "            f\"{label}: final episode={curve[-1]}, \"\n",
    "            f\"mean of last {len(last50)} episodes={last50.mean():.2f}\"\n",
    "        )\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for label, curve in all_curves:\n",
    "        plt.plot(curve, label=label, alpha=0.8)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Steps per Episode\")\n",
    "    plt.title(\"Convergence Curves for Different Tile Coding Configurations\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(out_path_fig)\n",
    "    plt.close()\n",
    "    print(f\"Convergence plot + data saved to {out_path_fig}, {out_path_csv}\")\n",
    "\n",
    "\n",
    "# Main Entry Point\n",
    "def main():\n",
    "    OUTPUT_DIR = \"lab5_outputs\"\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    DATA_DIR = os.path.join(OUTPUT_DIR, \"data\")\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "    # ---------- Baseline config: 8 tilings of 8x8 tiles ----------\n",
    "    env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "    obs_low = env.observation_space.low\n",
    "    obs_high = env.observation_space.high\n",
    "\n",
    "    NUM_TILINGS = 8\n",
    "    TILES_PER_DIM = 8\n",
    "\n",
    "    tilecoder = TileCoder(\n",
    "        obs_low=obs_low,\n",
    "        obs_high=obs_high,\n",
    "        num_tilings=NUM_TILINGS,\n",
    "        tiles_per_dim=TILES_PER_DIM,\n",
    "    )\n",
    "\n",
    "    agent = SemiGradientSarsaAgent(\n",
    "        env=env,\n",
    "        tilecoder=tilecoder,\n",
    "        alpha=0.1,         # scaled inside by num_tilings\n",
    "        gamma=1.0,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_min=0.01,\n",
    "        epsilon_decay=0.995,\n",
    "    )\n",
    "\n",
    "    NUM_EPISODES = 500\n",
    "    MAX_STEPS_PER_EPISODE = 200\n",
    "\n",
    "    print(\"=== Training baseline config: 8 tilings, 8x8 tiles ===\")\n",
    "    episode_lengths = agent.train(\n",
    "        num_episodes=NUM_EPISODES,\n",
    "        max_steps_per_episode=MAX_STEPS_PER_EPISODE,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    # ---------- Plots + Data for the baseline config ----------\n",
    "\n",
    "    # Learning curve\n",
    "    plot_learning_curve(\n",
    "        episode_lengths,\n",
    "        out_path_fig=os.path.join(OUTPUT_DIR, \"learning_curve_steps_per_episode.png\"),\n",
    "        out_path_csv=os.path.join(DATA_DIR, \"learning_curve_steps_per_episode.csv\"),\n",
    "    )\n",
    "\n",
    "    # Value function heatmap\n",
    "    plot_value_function(\n",
    "        agent,\n",
    "        tilecoder,\n",
    "        out_path_fig=os.path.join(OUTPUT_DIR, \"value_function_heatmap.png\"),\n",
    "        out_path_npz=os.path.join(DATA_DIR, \"value_function_grid.npz\"),\n",
    "        num_points_per_dim=60,\n",
    "    )\n",
    "\n",
    "    # Policy map\n",
    "    plot_policy(\n",
    "        agent,\n",
    "        tilecoder,\n",
    "        out_path_fig=os.path.join(OUTPUT_DIR, \"policy_map.png\"),\n",
    "        out_path_npy=os.path.join(DATA_DIR, \"policy_grid.npy\"),\n",
    "        num_points_per_dim=60,\n",
    "    )\n",
    "\n",
    "    # Trajectories over value function\n",
    "    plot_trajectories_over_value(\n",
    "        agent,\n",
    "        tilecoder,\n",
    "        out_path_fig=os.path.join(OUTPUT_DIR, \"trajectories_over_value.png\"),\n",
    "        out_path_npz=os.path.join(DATA_DIR, \"trajectories_over_value.npz\"),\n",
    "        num_traj=5,\n",
    "        max_steps=200,\n",
    "        num_points_per_dim=60,\n",
    "    )\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    # ---------- Convergence curves for different feature configs ----------\n",
    "    experiment_feature_configs(\n",
    "        env_name=\"MountainCar-v0\",\n",
    "        configs=[\n",
    "            {\"label\": \"4 tilings, 4x4\", \"num_tilings\": 4, \"tiles_per_dim\": 4},\n",
    "            {\"label\": \"8 tilings, 8x8\", \"num_tilings\": 8, \"tiles_per_dim\": 8},\n",
    "            {\"label\": \"16 tilings, 8x8\", \"num_tilings\": 16, \"tiles_per_dim\": 8},\n",
    "        ],\n",
    "        num_episodes=300,\n",
    "        max_steps=200,\n",
    "        out_path_fig=os.path.join(OUTPUT_DIR, \"convergence_feature_configs.png\"),\n",
    "        out_path_csv=os.path.join(DATA_DIR, \"convergence_feature_configs.csv\"),\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTraining complete. All figures in: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "    print(f\"Numeric data for graphs in: {os.path.abspath(DATA_DIR)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA EXPERIMENTS (SEPARATE FROM MAIN):\n",
    " - Compare epsilon schedules\n",
    " - Compare learning rates (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epsilon schedule experiment: Fast decay (0.99) ===\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "\n",
      "=== Epsilon schedule experiment: Medium decay (0.995) ===\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "\n",
      "=== Epsilon schedule experiment: Slow decay (0.999) ===\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "\n",
      "=== Epsilon schedule convergence summary ===\n",
      "Fast decay (0.99): final=146.80, last-50 mean=153.96\n",
      "Medium decay (0.995): final=155.20, last-50 mean=175.45\n",
      "Slow decay (0.999): final=200.00, last-50 mean=200.00\n",
      "Epsilon schedule results saved to lab5_outputs\\epsilon_schedules_convergence.png, lab5_outputs\\data\\epsilon_schedules_convergence.csv\n",
      "\n",
      "=== Step-size experiment: Small alpha (0.05) (alpha=0.05) ===\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "\n",
      "=== Step-size experiment: Medium alpha (0.10) (alpha=0.1) ===\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "\n",
      "=== Step-size experiment: Large alpha (0.20) (alpha=0.2) ===\n",
      "  Run 1/5\n",
      "  Run 2/5\n",
      "  Run 3/5\n",
      "  Run 4/5\n",
      "  Run 5/5\n",
      "\n",
      "=== Alpha convergence summary ===\n",
      "Small alpha (0.05): final=178.80, last-50 mean=189.85\n",
      "Medium alpha (0.10): final=183.20, last-50 mean=171.86\n",
      "Large alpha (0.20): final=157.00, last-50 mean=166.32\n",
      "Alpha experiment results saved to lab5_outputs\\alpha_schedules_convergence.png, lab5_outputs\\data\\alpha_schedules_convergence.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "\n",
    "# plt.ioff()  # turn off interactive mode so plt.show() doesn't interfere with saving\n",
    "\n",
    "# Helper: Epsilon Schedule Experiment\n",
    "def experiment_epsilon_schedules(\n",
    "    env_name,\n",
    "    eps_configs,\n",
    "    num_runs=5,\n",
    "    num_episodes=300,\n",
    "    max_steps=200,\n",
    "    out_path_fig=\"epsilon_schedules_convergence.png\",\n",
    "    out_path_csv=\"epsilon_schedules_convergence.csv\",\n",
    "):\n",
    "    all_means = []\n",
    "    labels = []\n",
    "\n",
    "    for cfg in eps_configs:\n",
    "        label = cfg[\"label\"]\n",
    "        print(f\"\\n=== Epsilon schedule experiment: {label} ===\")\n",
    "\n",
    "        runs = []\n",
    "        for run in range(num_runs):\n",
    "            print(f\"  Run {run + 1}/{num_runs}\")\n",
    "\n",
    "            env = gym.make(env_name)\n",
    "            tilecoder = TileCoder(\n",
    "                obs_low=env.observation_space.low,\n",
    "                obs_high=env.observation_space.high,\n",
    "                num_tilings=8,\n",
    "                tiles_per_dim=8,\n",
    "            )\n",
    "\n",
    "            agent = SemiGradientSarsaAgent(\n",
    "                env=env,\n",
    "                tilecoder=tilecoder,\n",
    "                alpha=0.1,\n",
    "                gamma=1.0,\n",
    "                epsilon_start=cfg[\"epsilon_start\"],\n",
    "                epsilon_min=cfg[\"epsilon_min\"],\n",
    "                epsilon_decay=cfg[\"epsilon_decay\"],\n",
    "            )\n",
    "\n",
    "            ep_lengths = agent.train(\n",
    "                num_episodes=num_episodes,\n",
    "                max_steps_per_episode=max_steps,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            runs.append(np.array(ep_lengths, dtype=np.int32))\n",
    "            env.close()\n",
    "\n",
    "        mean_curve = np.stack(runs, axis=0).mean(axis=0)\n",
    "        all_means.append(mean_curve)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Save numeric results\n",
    "    max_len = max(len(m) for m in all_means)\n",
    "    data = np.full((max_len, len(all_means) + 1), np.nan)\n",
    "    data[:, 0] = np.arange(1, max_len + 1)\n",
    "    header_cols = [\"episode\"]\n",
    "\n",
    "    for j, (label, mean_curve) in enumerate(zip(labels, all_means), start=1):\n",
    "        data[: len(mean_curve), j] = mean_curve\n",
    "        header_cols.append(label.replace(\",\", \"_\"))\n",
    "\n",
    "    np.savetxt(out_path_csv, data, delimiter=\",\",\n",
    "               header=\",\".join(header_cols), comments=\"\")\n",
    "\n",
    "    print(\"\\n=== Epsilon schedule convergence summary ===\")\n",
    "    for label, mean_curve in zip(labels, all_means):\n",
    "        last50 = mean_curve[-50:]\n",
    "        print(f\"{label}: final={mean_curve[-1]:.2f}, last-50 mean={last50.mean():.2f}\")\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for label, mean_curve in zip(labels, all_means):\n",
    "        plt.plot(mean_curve, label=label)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Mean Steps per Episode\")\n",
    "    plt.title(\"Convergence for Different Epsilon Schedules\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(out_path_fig)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Epsilon schedule results saved to {out_path_fig}, {out_path_csv}\")\n",
    "\n",
    "\n",
    "# Helper: Alpha (Learning Rate) Experiment\n",
    "def experiment_step_sizes(\n",
    "    env_name,\n",
    "    alpha_configs,\n",
    "    num_runs=5,\n",
    "    num_episodes=300,\n",
    "    max_steps=200,\n",
    "    out_path_fig=\"alpha_schedules_convergence.png\",\n",
    "    out_path_csv=\"alpha_schedules_convergence.csv\",\n",
    "):\n",
    "    all_means = []\n",
    "    labels = []\n",
    "\n",
    "    for cfg in alpha_configs:\n",
    "        label = cfg[\"label\"]\n",
    "        base_alpha = cfg[\"alpha\"]\n",
    "        print(f\"\\n=== Step-size experiment: {label} (alpha={base_alpha}) ===\")\n",
    "\n",
    "        runs = []\n",
    "        for run in range(num_runs):\n",
    "            print(f\"  Run {run + 1}/{num_runs}\")\n",
    "\n",
    "            env = gym.make(env_name)\n",
    "            tilecoder = TileCoder(\n",
    "                obs_low=env.observation_space.low,\n",
    "                obs_high=env.observation_space.high,\n",
    "                num_tilings=8,\n",
    "                tiles_per_dim=8,\n",
    "            )\n",
    "\n",
    "            agent = SemiGradientSarsaAgent(\n",
    "                env=env,\n",
    "                tilecoder=tilecoder,\n",
    "                alpha=base_alpha,\n",
    "                gamma=1.0,\n",
    "                epsilon_start=1.0,\n",
    "                epsilon_min=0.01,\n",
    "                epsilon_decay=0.995,\n",
    "            )\n",
    "\n",
    "            ep_lengths = agent.train(\n",
    "                num_episodes=num_episodes,\n",
    "                max_steps_per_episode=max_steps,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            runs.append(np.array(ep_lengths, dtype=np.int32))\n",
    "            env.close()\n",
    "\n",
    "        mean_curve = np.stack(runs, axis=0).mean(axis=0)\n",
    "        all_means.append(mean_curve)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Save numeric results\n",
    "    max_len = max(len(m) for m in all_means)\n",
    "    data = np.full((max_len, len(all_means) + 1), np.nan)\n",
    "    data[:, 0] = np.arange(1, max_len + 1)\n",
    "    header_cols = [\"episode\"]\n",
    "\n",
    "    for j, (label, mean_curve) in enumerate(zip(labels, all_means), start=1):\n",
    "        data[: len(mean_curve), j] = mean_curve\n",
    "        header_cols.append(label.replace(\",\", \"_\"))\n",
    "\n",
    "    np.savetxt(out_path_csv, data, delimiter=\",\",\n",
    "               header=\",\".join(header_cols), comments=\"\")\n",
    "\n",
    "    print(\"\\n=== Alpha convergence summary ===\")\n",
    "    for label, mean_curve in zip(labels, all_means):\n",
    "        last50 = mean_curve[-50:]\n",
    "        print(f\"{label}: final={mean_curve[-1]:.2f}, last-50 mean={last50.mean():.2f}\")\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for label, mean_curve in zip(labels, all_means):\n",
    "        plt.plot(mean_curve, label=label)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Mean Steps per Episode\")\n",
    "    plt.title(\"Convergence for Different Step Sizes (alpha)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(out_path_fig)\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"Alpha experiment results saved to {out_path_fig}, {out_path_csv}\")\n",
    "\n",
    "\n",
    "# RUN BOTH EXPERIMENTS (separate from main)\n",
    "OUTPUT_DIR = \"lab5_outputs\"\n",
    "DATA_DIR = os.path.join(OUTPUT_DIR, \"data\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# 1. Epsilon Schedules\n",
    "experiment_epsilon_schedules(\n",
    "    env_name=\"MountainCar-v0\",\n",
    "    eps_configs=[\n",
    "        {\"label\": \"Fast decay (0.99)\", \"epsilon_start\": 1.0, \"epsilon_min\": 0.01, \"epsilon_decay\": 0.99},\n",
    "        {\"label\": \"Medium decay (0.995)\", \"epsilon_start\": 1.0, \"epsilon_min\": 0.01, \"epsilon_decay\": 0.995},\n",
    "        {\"label\": \"Slow decay (0.999)\", \"epsilon_start\": 1.0, \"epsilon_min\": 0.01, \"epsilon_decay\": 0.999},\n",
    "    ],\n",
    "    num_runs=5,\n",
    "    num_episodes=300,\n",
    "    max_steps=200,\n",
    "    out_path_fig=os.path.join(OUTPUT_DIR, \"epsilon_schedules_convergence.png\"),\n",
    "    out_path_csv=os.path.join(DATA_DIR, \"epsilon_schedules_convergence.csv\"),\n",
    ")\n",
    "\n",
    "\n",
    "# 2. Step Sizes (alpha)\n",
    "experiment_step_sizes(\n",
    "    env_name=\"MountainCar-v0\",\n",
    "    alpha_configs=[\n",
    "        {\"label\": \"Small alpha (0.05)\", \"alpha\": 0.05},\n",
    "        {\"label\": \"Medium alpha (0.10)\", \"alpha\": 0.10},\n",
    "        {\"label\": \"Large alpha (0.20)\", \"alpha\": 0.20},\n",
    "    ],\n",
    "    num_runs=5,\n",
    "    num_episodes=300,\n",
    "    max_steps=200,\n",
    "    out_path_fig=os.path.join(OUTPUT_DIR, \"alpha_schedules_convergence.png\"),\n",
    "    out_path_csv=os.path.join(DATA_DIR, \"alpha_schedules_convergence.csv\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Experiments Summary\n",
    "\n",
    "As an additional part of the lab, I ran two more experiments to understand how exploration and learning-rate settings affect semi-gradient SARSA when using tile coding. First, I compared three ε-decay schedules (fast, medium, slow) because ε-greedy controls how often the agent explores versus exploits. A fast decay reduces exploration quickly, a slow decay keeps exploration high for longer, and a medium decay sits in between. The results showed that the fast decay (0.99) performed the best overall with a final mean of 166.6 and a last-50 average of 151.14, meaning it balanced exploration and exploitation well. The medium decay (0.995) learned slower and was less stable, while the slow decay (0.999) never improved and stayed at 200 steps, showing that too much exploration prevents the agent from settling on a useful strategy. Second, I tested different learning rates (α = 0.05, 0.10, 0.20) because step size strongly affects stability in function approximation. The small α barely improved, and the agent stayed near 190 steps, meaning it learned too slowly. The medium α performed somewhat better at around 176 steps, and the large α (0.20) gave the best results with a final mean of 155.8, although higher step sizes can sometimes cause instability. Overall, these extra experiments helped show how sensitive MountainCar is to exploration schedules and step sizes, and they highlight the balance needed between learning speed and stability when using semi-gradient methods with tile coding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "OpenAI. (2025). ChatGPT (Version 5.1) [Large language model]. https://chat.openai.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
